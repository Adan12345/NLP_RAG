# -*- coding: utf-8 -*-
"""A4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12tBXEFmRsvwsHc1hC0yPBm-nF6cAcBRv
"""

import streamlit as st
import zipfile
import json
import os
import re
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from transformers import T5ForConditionalGeneration, T5Tokenizer
import faiss
from typing import List, Dict
from math import sqrt

# ---------------------------------------------------------
# Helper: Query expansion (clinical synonyms & symptoms)
# ---------------------------------------------------------
def expand_query(query: str) -> str:
    q = query.lower()
    expansions = {
        "blood pressure": ["hypertension", "high blood pressure", "high bp"],
        "heart attack": ["myocardial infarction", "mi", "cardiac infarction"],
        "sugar": ["glucose", "blood sugar", "hyperglycemia"],
        "fever": ["fever", "chills", "headache", "muscle ache", "malaise", "sweating"],
        "fever and cough": ["respiratory infection", "pneumonia", "covid"],
        # add more as needed
    }
    extra = []
    for key, vals in expansions.items():
        if key in q:
            extra.extend(vals)
    if extra:
        q = q + " " + " ".join(extra)
    return q

# ---------------------------------------------------------
# 1. Load & Extract ZIP â†’ JSON â†’ Document Strings
# ---------------------------------------------------------
@st.cache_data(show_spinner=True)
def load_zip_and_build_documents(zip_path: str, chunk_size: int = 500) -> List[str]:
    if not os.path.exists(zip_path):
        st.error(f"ZIP file not found: {zip_path}")
        st.stop()

    documents = []
    total_json_files = 0
    total_fields = set()

    with zipfile.ZipFile(zip_path, 'r') as z:
        json_files = [f for f in z.namelist() if f.endswith(".json")]
        total_json_files = len(json_files)

        if len(json_files) == 0:
            st.error("No JSON files found inside the ZIP.")
            st.stop()

        for jf in json_files:
            try:
                with z.open(jf) as f:
                    data = json.load(f)
                    total_fields.update(data.keys())
                    # Flatten JSON into text
                    doc_parts = []
                    for key, value in data.items():
                        if value is None:
                            continue
                        value = str(value)
                        value = re.sub(r"\s+", " ", value)
                        doc_parts.append(f"{key}: {value}")

                    full_text = " | ".join(doc_parts)
                    # Split into chunks to improve retrieval
                    for i in range(0, len(full_text), chunk_size):
                        chunk = full_text[i:i+chunk_size]
                        if len(chunk) > 20:
                            documents.append(chunk)

            except Exception as e:
                st.warning(f"Could not read {jf}: {e}")

    if len(documents) == 0:
        st.error("ZIP extracted but no valid documents were generated.")
        st.stop()

    avg_chunk_len = np.mean([len(d) for d in documents])
    return documents, total_json_files, len(total_fields), avg_chunk_len

# ---------------------------------------------------------
# 2. Hybrid Retriever (BM25 + Dense + FAISS)
# ---------------------------------------------------------
@st.cache_resource(show_spinner=True)
class HybridRetriever:
    def __init__(self, documents: List[str]):
        self.documents = documents
        self.model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

        # BM25
        tokenized = [doc.lower().split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized)

        # Dense embeddings (normalized)
        embeddings = self.model.encode(documents, convert_to_numpy=True)
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        norms[norms == 0] = 1.0
        embeddings = embeddings / norms
        self.embeddings = embeddings.astype("float32")

        # FAISS index (cosine via inner product)
        d = self.embeddings.shape[1]
        self.index = faiss.IndexFlatIP(d)
        self.index.add(self.embeddings)

    def _normalize_bm25(self, scores: np.ndarray) -> np.ndarray:
        if np.all(np.isclose(scores, 0)):
            return scores
        min_s = float(np.min(scores))
        max_s = float(np.max(scores))
        if max_s == min_s:
            return np.zeros_like(scores)
        return (scores - min_s) / (max_s - min_s)

    def retrieve(self, query: str, k=5, alpha=0.5):
        expanded_query = expand_query(query)

        q_emb = self.model.encode([expanded_query], convert_to_numpy=True)
        qnorm = np.linalg.norm(q_emb)
        if qnorm == 0:
            qnorm = 1.0
        q_emb = (q_emb / qnorm).astype("float32")

        search_k = max(k, 10)
        D, I = self.index.search(q_emb, search_k)
        dense_scores = (D[0] + 1) / 2  # [-1,1] â†’ [0,1]

        bm25_raw = self.bm25.get_scores(expanded_query.lower().split())
        bm25_norm = self._normalize_bm25(bm25_raw)

        bm25_topk = np.argsort(bm25_raw)[::-1][:search_k]
        candidate_idxs = list(dict.fromkeys(list(I[0]) + list(bm25_topk)))

        combined = []
        for idx in candidate_idxs:
            dense_score = float(dense_scores[list(I[0]).index(idx)]) if idx in I[0] else 0.0
            sparse_score = float(bm25_norm[idx]) if idx < len(bm25_norm) else 0.0
            combined_score = (1 - alpha) * dense_score + alpha * sparse_score
            combined.append((idx, combined_score, dense_score, sparse_score))

        combined = sorted(combined, key=lambda x: x[1], reverse=True)[:k]

        results = []
        for idx, comb_s, dense_s, sparse_s in combined:
            results.append({
                "index": int(idx),
                "document": self.documents[int(idx)],
                "score": float(comb_s),
                "dense_score": float(dense_s),
                "sparse_score": float(sparse_s)
            })
        return results

# ---------------------------------------------------------
# 3. Generator (FLAN-T5) â€” full answer prompt
# ---------------------------------------------------------
@st.cache_resource(show_spinner=True)
class RAGGenerator:
    def __init__(self):
        self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")
        self.model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)

    def generate(self, query: str, docs: List[Dict], relevance_threshold: float = 0.35):
        scores = [d.get("score", 0.0) for d in docs] if docs else [0.0]
        max_score = max(scores) if scores else 0.0

        if max_score < relevance_threshold:
            prompt = f"""
You are a certified medical expert. The retrieved documents are not sufficiently relevant.
Answer the user's question using reliable medical knowledge.

- List all common symptoms or features clearly.
- Use bullet points if possible.
- Be concise and accurate.
- Mention uncertainty if any.

Question: {query}

Answer:
"""
        else:
            context = "\n\n".join([f"[Doc {d['index']} | score={d['score']:.3f}]\n{d['document']}" for d in docs])
            prompt = f"""
You are a highly qualified clinical and medical expert.

- Use the context below to answer the question fully.
- List all relevant symptoms, definitions, or clinical features.
- If the context is incomplete, supplement with reliable medical knowledge.
- Use bullet points if applicable.
- Do NOT hallucinate information.

Context:
{context}

Question: {query}

Answer:
"""

        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024
        ).to(self.device)

        output = self.model.generate(
            **inputs,
            max_new_tokens=400,  # increased for longer answers
            num_beams=4,
            early_stopping=True
        )

        answer = self.tokenizer.decode(output[0], skip_special_tokens=True)
        return answer, max_score

# ---------------------------------------------------------
# 4. Full RAG System
# ---------------------------------------------------------
class DirectRAGSystem:
    def __init__(self, retriever: HybridRetriever, generator: RAGGenerator):
        self.retriever = retriever
        self.generator = generator

    def query(self, query: str, k=5, alpha=0.5, relevance_threshold: float = 0.35):
        docs = self.retriever.retrieve(query, k=k, alpha=alpha)
        answer, max_score = self.generator.generate(query, docs, relevance_threshold=relevance_threshold)
        return {"answer": answer, "retrieved_documents": docs, "max_relevance": max_score}

# ---------------------------------------------------------
# 5. Streamlit UI
# ---------------------------------------------------------
st.set_page_config(page_title="Clinical RAG from ZIP", page_icon="ðŸ¥", layout="wide")
st.title("ðŸ¥ ZIP-Based Clinical RAG System (Improved)")

st.info("Upload your ZIP file containing JSON files to build the RAG system.")

uploaded_file = st.file_uploader("Upload ZIP file", type=["zip"])

if uploaded_file is not None:
    temp_zip_path = "uploaded_dataset.zip"
    with open(temp_zip_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    if st.button("Load ZIP Dataset"):
        with st.spinner("Extracting and building documents..."):
            docs, num_json, num_fields, avg_chunk_len = load_zip_and_build_documents(temp_zip_path)

        st.success(f"Loaded {len(docs)} document chunks from {num_json} JSON files!")
        st.markdown(f"- **Unique JSON fields:** {num_fields}  \n- **Average chunk length:** {avg_chunk_len:.1f} chars")

        with st.spinner("Initializing Hybrid Retriever..."):
            retriever = HybridRetriever(docs)

        with st.spinner("Loading FLAN-T5 model..."):
            generator = RAGGenerator()

        rag = DirectRAGSystem(retriever, generator)
        st.session_state["rag"] = rag
        st.success("System is ready!")

# Query interface
if "rag" in st.session_state:
    st.markdown("---")
    query = st.text_input("Enter your clinical query:")

    col1, col2, col3 = st.columns(3)
    with col1:
        top_k = st.slider("Top-K Documents", 1, 10, 5)
    with col2:
        alpha = st.slider("Hybrid Weight (BM25 vs Dense)", 0.0, 1.0, 0.3)
    with col3:
        relevance_threshold = st.slider("Relevance threshold (0-1)", 0.0, 1.0, 0.35)

    if st.button("Search"):
        rag = st.session_state["rag"]
        expanded_q = expand_query(query)

        with st.spinner("Thinking..."):
            result = rag.query(expanded_q, k=top_k, alpha=alpha, relevance_threshold=relevance_threshold)

        st.subheader("Answer")
        st.success(result["answer"])

        st.subheader("Retrieved Documents (top results)")
        if result["retrieved_documents"]:
            st.write(f"Max combined relevance score: {result['max_relevance']:.3f}")
            for d in result["retrieved_documents"]:
                st.write(f"**Doc {d['index']} | combined:** {d['score']:.4f}  | dense: {d['dense_score']:.4f} | sparse: {d['sparse_score']:.4f}")
                st.code(d["document"])
        else:
            st.write("No documents retrieved.")

# ---------------------------------------------------------
# 6. Evaluation & Ethical Considerations
# ---------------------------------------------------------
st.markdown("---")
st.subheader("Evaluation & Ethical Considerations")

test_queries = {
    "patient has fever and cough": [0, 2],
    "diabetic patient blood sugar": [5, 6],
}
if "rag" in st.session_state:
    retriever = st.session_state["rag"].retriever
    for q, expected_indices in test_queries.items():
        retrieved = retriever.retrieve(q, k=3)
        retrieved_indices = [d["index"] for d in retrieved]
        precision = len(set(retrieved_indices) & set(expected_indices)) / len(retrieved_indices) if retrieved_indices else 0.0
        recall = len(set(retrieved_indices) & set(expected_indices)) / len(expected_indices) if expected_indices else 0.0
        st.write(f"- Query: `{q}` â†’ Precision@3: {precision:.2f}, Recall@3: {recall:.2f}")

st.markdown("""
**Ethical & Privacy Checklist**
- Ensure patient data is anonymized.
- Comply with HIPAA/GDPR if using real data.
- Do not expose sensitive information in generated answers.
""")